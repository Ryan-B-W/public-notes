:PROPERTIES:
:ID:       91af8918-9413-41f4-a29b-848b1edae85e
:ROAM_REFS: https://www.researchgate.net/publication/387540631_The_Impact_of_Prompt_Programming_on_Function-Level_Code_Generation
:END:
#+title: (www.researchgate.net) (PDF) The Impact of Prompt Programming on Function-Level Code Generation
#+filetags: :nlp:artificial_intelligence:programming:software_development:computer_science:article:science_paper:website:

#+begin_quote
  * The Impact of Prompt Programming on Function-Level Code Generation

  - December 2024

  DOI:[[http://dx.doi.org/10.48550/arXiv.2412.20545][10.48550/arXiv.2412.20545]]

  Authors:

  [[https://www.researchgate.net/scientific-contributions/Ranim-Khojah-2256035788][Ranim Khojah]]

  [[https://www.researchgate.net/scientific-contributions/Francisco-Gomes-de-Oliveira-Neto-2181795201][Francisco Gomes de Oliveira Neto]]

  [[https://www.researchgate.net/profile/Mazen-Mohamad][Mazen Mohamad]]

  - [[https://www.researchgate.net/institution/RISE_Research_Institutes_of_Sweden][RISE Research Institutes of Sweden]]

  [[https://www.researchgate.net/profile/Philipp-Leitner-3][Philipp Leitner]]

  - [[https://www.researchgate.net/institution/Chalmers-University-of-Technology][Chalmers University of Technology]]

  ** Abstract and Figures

  Large Language Models (LLMs) are increasingly used by software engineers for code generation.  However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code.  Despite this, the impact of different prompt techniques – and their combinations – on code generation remains underexplored.  In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome.  Additionally, we observed a trade-off between correctness and quality when using prompt techniques.  Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.
#+end_quote
* promptContext.pdf                               :mime_type_application_pdf:
:PROPERTIES:
:ID:       8d793c01-cf29-4cd0-9d01-f2e61fc24d4d
:END:

#+begin_quote
  * The Impact of Prompt Programming on Function-Level Code Generation

  Ranim Khojah, Francisco Gomes de Oliveira Neto, Mazen Mohamad, Philipp Leitner

  Chalmers University of Technology and University of Gothenburg, 2RISE Research Institutes of Sweden Gothenburg, Sweden

  khojah@chalmers.se, francisco.gomes@cse.gu.se, mazen.mohamad@ri.se, philipp.leitner@chalmers.se

  ** Abstract

  Large Language Models (LLMs) are increasingly used by software engineers for code generation.  However, limitations of LLMs such as irrelevant or incorrect code have highlighted the need for prompt programming (or prompt engineering) where engineers apply specific prompt techniques (e.g., chain-of-thought or input-output examples) to improve the generated code.  Despite this, the impact of different prompt techniques — and their combinations — on code generation remains underexplored.  In this study, we introduce CodePromptEval, a dataset of 7072 prompts designed to evaluate five prompt techniques (few-shot, persona, chain-of-thought, function signature, list of packages) and their effect on the correctness, similarity, and quality of complete functions generated by three LLMs (GPT-4o, Llama3, and Mistral). Our findings show that while certain prompt techniques significantly influence the generated code, combining multiple techniques does not necessarily improve the outcome.  Additionally, we observed a trade-off between correctness and quality when using prompt techniques.  Our dataset and replication package enable future research on improving LLM-generated code and evaluating new prompt techniques.
#+end_quote
* (www.researchgate.net) The percentages of error types that we observed in failed functions... | Download Scientific Diagram
:PROPERTIES:
:ID:       e6bb280c-e6ec-4cd3-889c-285099b9e7ec
:ROAM_REFS: https://www.researchgate.net/figure/The-percentages-of-error-types-that-we-observed-in-failed-functions-generated-by_fig6_387540631
:END:

#+begin_quote
  The percentages of error types that we observed in failed functions generated by different combinations of prompt techniques (Llama3).
#+end_quote
